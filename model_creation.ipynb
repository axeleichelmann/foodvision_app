{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Testing Creation Notebook\n",
    "\n",
    "In this notebook we will be creating and testing different models to use for food label prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - Import Necessary Libraries & Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "\n",
    "# Import extra libraries\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///tmp/mlflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///tmp/mlflow/394942238179406615', creation_time=1730383309214, experiment_id='394942238179406615', last_update_time=1730383309214, lifecycle_stage='active', name='FoodVision_Replica', tags={}>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up MLFlow configuration\n",
    "import mlflow\n",
    "MODEL_REGISTRY = Path(\"/tmp/mlflow\")\n",
    "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
    "MLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print (mlflow.get_tracking_uri())\n",
    "mlflow.set_experiment(\"FoodVision_Replica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Food101 Dataset to 'data' folder from torchvision\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "train_data = torchvision.datasets.Food101(root=data_dir, split=\"train\",\n",
    "                                        download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_data = torchvision.datasets.Food101(root=data_dir, split=\"test\",\n",
    "                                    download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_dataloader, optimizer, loss_fn):\n",
    "    \"\"\"Carry out training step for one epoch\"\"\"\n",
    "\n",
    "    loss, acc = 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # Make predictions\n",
    "        y_logits = model(images)  # Forward propagation\n",
    "        y_probs = y_logits.softmax(dim=1)  # Calculate predicted probabilities\n",
    "\n",
    "        batch_loss = loss_fn(y_logits, labels)  # Calculate loss of predicitions\n",
    "        batch_acc = (y_probs.argmax(dim=1) == labels).sum().item() / len(y_probs)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()   # Reset optimizer gradient\n",
    "        batch_loss.backward()   # Carry out back-propagation\n",
    "        optimizer.step()   # Update model weights\n",
    "\n",
    "        loss += (batch_loss.item() - loss) / (i+1)  # Update cumulative epoch loss\n",
    "        acc += (batch_acc - acc) / (i+1)  # Update cumulative epoch accuracy\n",
    "\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, val_dataloader, loss_fn):\n",
    "    \"\"\"Carry out the evaluation step for an epoch of training\"\"\"\n",
    "    loss, acc = 0.0, 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i, (images, labels) in enumerate(val_dataloader):\n",
    "            \n",
    "            # Calculate predictions, predictions loss, & predictions accuracy\n",
    "            y_logits = model(images)    # Carry out Forward Propagation\n",
    "            y_probs = y_logits.softmax(dim=1)\n",
    "            batch_loss = loss_fn(y_logits, labels)    # Calculate batch loss\n",
    "            batch_acc = (y_probs.argmax(dim=1) == labels).sum().item()/len(y_logits) # Calculate batch accuracy\n",
    "\n",
    "            # Update cumulative loss\n",
    "            loss += (batch_loss.detach().item() - loss) / (i + 1)\n",
    "            acc += (batch_acc - acc)/(i+1)\n",
    "            \n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, train_dataloader, val_dataloader, optimizer, loss_fn):\n",
    "    \"\"\"Carry out entire model training process for the specified number of epochs\"\"\"\n",
    "\n",
    "    # Create lists in which to store training & testing loss and accuracy\n",
    "    train_losses, val_losses = [], []  \n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        # Get training loss & accuracy\n",
    "        training_loss, training_acc = train_step(model, train_dataloader, optimizer, loss_fn)\n",
    "        train_losses.append(training_loss)\n",
    "        train_accuracies.append(training_acc)\n",
    "        \n",
    "        # Get validation loss & accuracy\n",
    "        validation_loss, validation_acc = eval_step(model, val_dataloader, loss_fn)\n",
    "        val_losses.append(validation_loss)\n",
    "        val_accuracies.append(validation_acc)\n",
    "\n",
    "    return_dict = {\"training_losses\" : train_losses, \"validation_losses\" : val_losses,\n",
    "                   \"training_accuracies\" : train_accuracies, \"validation_accuracies\" : val_accuracies}\n",
    "    \n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_losses : list, val_losses : list):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss curves\n",
    "    \n",
    "    Args:\n",
    "    train_losses (list) : list containing the training loss for each epoch\n",
    "    val_losses (list) : list containing the validation loss for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(6,4))\n",
    "    ax[0].plot(train_losses)\n",
    "    ax[0].set_title(\"Training Loss Curve\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    ax[1].plot(val_losses)\n",
    "    ax[1].set_title(\"Validation Loss Curve\")\n",
    "    ax[1].set_ylabel(\"Loss\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curves(train_accs : list, val_accs : list):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss curves\n",
    "    \n",
    "    Args:\n",
    "    train_losses (list) : list containing the training loss for each epoch\n",
    "    val_losses (list) : list containing the validation loss for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(6,4))\n",
    "    ax[0].plot(train_accs)\n",
    "    ax[0].set_title(\"Training Accuracy Curve\")\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    ax[1].plot(val_accs)\n",
    "    ax[1].set_title(\"Validation Accuracy Curve\")\n",
    "    ax[1].set_ylim(0,1)\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb0_classifier(out_features):\n",
    "    # Import EfficientNet B0 model architecture & assign it default weights\n",
    "    model = torchvision.models.efficientnet_b0()\n",
    "    for param in model.parameters():  # Freeze internal parameters\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Adjust classifier layer\n",
    "    model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(in_features=1280, out_features=out_features))\n",
    "    return model\n",
    "\n",
    "def effnet_mlflow_run(name, num_classes, train_params,\n",
    "                      train_dataloader, val_dataloader):\n",
    "    \n",
    "    with mlflow.start_run(run_name=name):\n",
    "        print(f\"'{name}' run started\")\n",
    "\n",
    "        # Create model\n",
    "        model = create_effnetb0_classifier(out_features=num_classes)\n",
    "\n",
    "        # Set up optimizer & loss function\n",
    "        optimizer = train_params[\"optimizer\"](params=model.parameters(), lr=train_params[\"lr\"])\n",
    "        loss_fn = train_params[\"loss_fn\"]()\n",
    "\n",
    "        mlflow.set_tag(\"model_name\", \"EfficientNetB0\")\n",
    "        mlflow.log_params(train_params)\n",
    "    \n",
    "        # Carry out model training\n",
    "        results = train_model(model=model, num_epochs=train_params[\"num_epochs\"],\n",
    "                            train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "                            optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "        model_metrics = {\"training_loss\" : max(results[\"training_losses\"]),\n",
    "                        \"validation_loss\" : max(results[\"validation_losses\"]),\n",
    "                        \"training_acc\" : max(results[\"training_accuracies\"]),\n",
    "                        \"validation_acc\" : max(results[\"validation_accuracies\"])}\n",
    "    \n",
    "        mlflow.log_metrics(model_metrics)\n",
    "        mlflow.pytorch.log_model(model, \"pytorch_models\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Baseline Model\n",
    "The Baseline Model we use will be an EfficientNet B0 Model w/ Adjusted Classifier trained on 20% of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 20% subset contains 15150 images\n",
      "Testing data 20% subset contains 5050 images\n"
     ]
    }
   ],
   "source": [
    "# Import EfficientNet B0 model pretrained weights & transformation\n",
    "effnet_b0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "effnet_b0_transforms = effnet_b0_weights.transforms()\n",
    "\n",
    "# Apply EfficientNet B0 Transformation to datasets\n",
    "train_data.transform = effnet_b0_transforms\n",
    "test_data.transform = effnet_b0_transforms\n",
    "\n",
    "# Create Subset of training data which consists of 20% of the training images (150 images) for each class\n",
    "train_subset_length = round(0.2*len(train_data))\n",
    "train_data_subset, _ = torch.utils.data.random_split(train_data, lengths=[train_subset_length, len(train_data)-train_subset_length])\n",
    "print(f\"Training data 20% subset contains {len(train_data_subset)} images\")\n",
    "\n",
    "# Create subset of testing data consists of 20% of the training images (50 images) for each class\n",
    "test_subset_length = round(0.2*len(test_data))\n",
    "test_data_subset, _ = torch.utils.data.random_split(test_data, [test_subset_length, len(test_data) - test_subset_length])\n",
    "print(f\"Testing data 20% subset contains {len(test_data_subset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader out of training data 20% subset\n",
    "train_subset_dataloader = DataLoader(dataset=train_data_subset, batch_size=8,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=0)\n",
    "\n",
    "# Create dataloader out of testing data 20% subset\n",
    "test_subset_dataloader = DataLoader(dataset=test_data_subset, batch_size=8,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'effnetb0_base' run started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [21:51<43:43, 1311.73s/it]"
     ]
    }
   ],
   "source": [
    "mlflow.pytorch.autolog(disable=True)\n",
    "\n",
    "# Define training parameters\n",
    "train_params = {\"optimizer\" : torch.optim.SGD,\n",
    "                \"lr\" : 0.001,\n",
    "                \"num_epochs\" : 3,\n",
    "                \"loss_fn\" : nn.CrossEntropyLoss}\n",
    "\n",
    "model = effnet_mlflow_run(name=\"effnetb0_base\", num_classes=101, train_params=train_params,\n",
    "                          train_dataloader=train_subset_dataloader, val_dataloader=test_subset_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state dictionary\n",
    "SAVED_MODEL_DIR = Path(\"saved_models/\")\n",
    "SAVED_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), SAVED_MODEL_DIR / \"baseline_effnetb0\")\n",
    "\n",
    "# Check size of baseline model\n",
    "print(f\"The baseline EfficientNetB0 model's state dictionary is {os.path.getsize(SAVED_MODEL_DIR / \"baseline_effnetb0\")/(1024*1024)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
